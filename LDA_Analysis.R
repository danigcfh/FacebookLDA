# Load the required libraries
library(topicmodels)

# Load the data
all_training_data <- readRDS("all_training_data.rds")
all_pages_data <- readRDS("all_pages_data.rds")

# Function to print top terms for an LDA model
print_top_terms <- function(lda_model, n = 10) {
  cat("Top Terms:\n")
  terms_df <- as.data.frame(terms(lda_model, n))
  print(terms_df)
}

# Print top terms for LDA models in samples
for (sample_key in names(all_lda_models)) {
  for (lda_model_key in names(all_lda_models[[sample_key]])) {
    lda_model <- all_lda_models[[sample_key]][[lda_model_key]]
    cat("Sample:", sample_key, "Model:", lda_model_key, "\n")
    print_top_terms(lda_model)
  }
}

# Print top terms for LDA models in pages
for (page_key in names(pages_lda_models)) {
  for (lda_model_key in names(pages_lda_models[[page_key]]$models)) {
    lda_model <- pages_lda_models[[page_key]]$models[[lda_model_key]]$lda_model
    cat("Page:", page_key, "Model:", lda_model_key, "\n")
    print_top_terms(lda_model)
  }
}





# Usage:
coherence <- calculate_coherence(gen_lda_model, gen_dtm, gen_corpus)

# Use best_model for further analysis

# Load the testing data
test_data <- readRDS("test_data.rds")

# Extract the testing set
test_samples_dtm <- test_data$dtm
test_samples <- test_data$corpus
tests <- test_data$posts

# Initialize a list to store evaluation results
evaluation_results <- list()

# Iterate over each sample in the training data
for (sample_key in names(all_combined_data_list)) {
  # Get the combined data for the current sample
  combined_data <- all_combined_data_list[[sample_key]]
  
  # Iterate over each LDA model in the sample
  for (lda_model_key in names(combined_data)) {
    
    alpha_match <- regmatches(lda_model_key, regexec("alpha_([^_]+)", lda_model_key))
    k_match <- regmatches(lda_model_key, regexec("k_(\\d+)_alpha", lda_model_key))
    
    # Check if matches were found
    if (length(alpha_match[[1]]) == 2) {
      alpha_value <- as.numeric(alpha_match[[1]][2])
    } else {
      cat("No alpha match found for:", lda_model_key, "\n")
      alpha_value <- NA
    }
    
    if (length(k_match[[1]]) == 2) {
      k_value <- as.numeric(k_match[[1]][2])
    } else {
      cat("No k match found for:", lda_model_key, "\n")
      k_value <- NA
    }
    
    # Extract the model 
    lda_model <- combined_data[[lda_model_key]]$combined_data$lda_model

    # Extract topic assignments for each document
    topic_assignments <- posterior(lda_model)$topics
    
    # Save the evaluation results
    result_key <- paste(sample_key, lda_model_key, sep = "_")
    evaluation_results[[result_key]] <- list(
      test_perplexity = test_perplexity,
      coherence_scores = coherence_scores,
      alpha = alpha_match,
      k = k_match,
      sample = sample_key,
      topic_assignments = topic_assignments
    )
  }
}




# Save the evaluation results
saveRDS(evaluation_results, file = "evaluation_results.rds")

#When interpreting the results:
#Comparisons: Compare the test perplexity and coherence scores across different models to identify the one that performs better. A good model should have both low perplexity and high coherence.
#Trade-offs: Sometimes, there might be a trade-off between perplexity and coherence. It's essential to strike a balance between the two, as an overly complex model may have low perplexity but produce less interpretable topics.
#Domain Knowledge: Consider your domain knowledge and the interpretability of topics. If the topics generated by a model make more sense in the context of your data, it might be a better choice, even if the perplexity is slightly higher.
#Visualization: Visualize the topics generated by the best model using techniques such as word clouds, topic proportions, or other visualizations. This can provide a more intuitive understanding of the topics.
library(ggplot2)

# Initialize a data frame to store all coherence scores with metadata
all_coherence_data <- data.frame(sample_index = numeric(),
                                 k_value = numeric(),
                                 alpha_value = numeric(),
                                 coherence_score = numeric())


# Iterate over each result in the evaluation results
for (result_key in names(evaluation_results)) {
  # Extract metadata
  sample_index <- evaluation_results[[result_key]]$sample
  k_value <- evaluation_results[[result_key]]$k[[1]][2]  # Extract k value from the list
  alpha_value <- evaluation_results[[result_key]]$alpha[[1]][2]  # Extract alpha value from the list
  
  # Extract coherence scores
  coherence_scores <- evaluation_results[[result_key]]$coherence_scores
  
  # Create a data frame with metadata and coherence scores
  coherence_data <- data.frame(sample_index = rep(sample_index, length(coherence_scores)),
                               k_value = rep(k_value, length(coherence_scores)),
                               alpha_value = rep(alpha_value, length(coherence_scores)),
                               coherence_score = coherence_scores)
  
  # Combine with the overall data frame
  all_coherence_data <- rbind(all_coherence_data, coherence_data)
}

# Convert k_value and alpha_value to factors

summary(all_coherence_data)

# Create a heatmap
ggplot(all_coherence_data, aes(x = k_value, y = alpha_value, fill = coherence_score)) +
  geom_tile() +
  facet_wrap(~ sample_index) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Coherence Scores Across Samples, k, and Alpha")



# Load the combined data file
loaded_data <- readRDS("combined_data_sample_5.rds")

get_top_terms <- function(lda_model, topic, top_n = 10) {
  beta <- as.matrix(lda_model)[, terms(lda_model)]
  top_terms_idx <- order(-beta[topic, ])[1:top_n]
  return(terms(lda_model)[top_terms_idx])
}




# Check topics and terms for the first few models

terms(lda_models$k_3_alpha_0.1_sample_sample_1, 10)
names(all_lda_models$sample_1)
terms(all_lda_models$sample_1$k_3_alpha_0.1_sample_sample_1, 10)

cat("Checking Topics and Terms for sample:", sample_key, "\n")





# Identify the best hyperparameter combination based on coherence score
best_combination <- names(all_coherence_scores)[which.max(sapply(all_coherence_scores, max))]

# Access the best LDA model, DTM, and coherence scores
best_lda_model <- all_lda_models[[best_combination]]
best_dtm <- all_dtms[[best_combination]]
best_coherence_scores <- all_coherence_scores[[best_combination]]


associations <- findAssocs(gen_dtm, terms(gen_lda_model), 0.2)  # Adjust the threshold as needed


#This is to just keep the topics with max probability at the top
# Iterate over the list of LDA models
for (page in names(lda_models)) {
  cat("Page:", page, "\n")
  
  # Get the LDA model for the current page
  lda_model <- lda_models[[page]]
  
  # Get the terms and associated probabilities
  topic_terms <- terms(lda_model, 10)  # Adjust the number 10 based on the number of terms you want to display
  
  # Calculate the total probability for each topic
  topic_total_prob <- rowSums(lda_model@beta)
  
  # Order topics based on total probability in descending order
  ordered_topics <- order(topic_total_prob, decreasing = TRUE)
  
  # Print the topics and associated terms in the sorted order
  for (i in ordered_topics) {
    cat("Topic", i, ":", paste(topic_terms[i, ], collapse = ", "), "\n")
  }
  
  cat("\n")
}

# Load the training data
training_data <- readRDS("all_training_data.rds")

training_data[[sample_key]][[lda_model_key]]$combined_data$coherence_scores
training_data$sample_1$k_3_alpha_0.1_sample_sample_1$combined_data$coherence_scores
training_data$sample_1$k_5_alpha_0.1_sample_sample_1$combined_data$coherence_scores

# Initialize a vector to store all coherence scores
all_coherence_scores <- numeric()

# Iterate over samples in the loaded data
for (sample_key in names(training_data)) {
  sample_data <- training_data[[sample_key]]
  coherence_scores <- sample_data[[1]]$combined_data$coherence_scores
    # Append coherence scores to the vector
  all_coherence_scores <- c(all_coherence_scores, coherence_scores)
}
print(all_coherence_scores)

# Check for NAs in coherence scores
problematic_indices <- which(!is.finite(coherence_scores))

# Print the indices of problematic values
print(problematic_indices)

# Filter out infinite values
filtered_coherence_scores <- coherence_scores[is.finite(coherence_scores)]
# Create a boxplot for each sample
boxplot(filtered_coherence_scores, main = "Coherence Scores for LDA Models", xlab = "Models", ylab = "Coherence")
# Create an overall boxplot
boxplot(filtered_coherence_scores, main = "Overall Coherence Scores", ylab = "Coherence")



